{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture 11_ More on Data Manipulation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPct0iAZgQT6+hfRaFL+anX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lJo0lM9YX2T3"},"source":["----\n","#Pandas Idioms\n","----\n","\n","Python programmers will often suggest that there many ways python can be used to solve a particular problem. However, some are more appropriate than others. The best solutions are celebrated as \"**Idiomatic Python**\" and there are lots of great examples of this on StackOverflow and other websites. That is to say, the single quickest way to increase maintainability and decrease 'simple' bugs is to strive to write idiomatic Python. \n","\n","Pandas has its own set of idioms, akin to a sub-language within Python. We've alluded to some of these already, such as using vectorization whenever possible, and not using iterative loops if you don't need to.\n","\n","Several developers and users within the Panda's community have used the term **pandorable** for these idioms. I think it's a great term. So, I wanted to share with you a couple of key features of how you can make your code \"pandorable\".\n","\n","Let's start by calling in a dataset we can use to explore pandas' different idioms."]},{"cell_type":"code","metadata":{"id":"roo7y4BEX7Hn","executionInfo":{"status":"ok","timestamp":1639404365890,"user_tz":-240,"elapsed":20764,"user":{"displayName":"Bedoor AlShebli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDz9yYOu5OF7H8uFOwHv13QhCL-uS3bC2RbvY=s64","userId":"16007664423560638977"}},"outputId":"af9dcf8d-9e57-4b26-938c-1eccd0db237a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Mount the drive. \n","from google.colab import drive\n","drive.mount('/content/drive')\n","#!ls /content/drive/My\\ Drive/Applied\\ Data\\ Science\\ in\\ Python/datasets/  "],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"FusNxqB6X2T8","executionInfo":{"status":"error","timestamp":1639404369207,"user_tz":-240,"elapsed":422,"user":{"displayName":"Bedoor AlShebli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDz9yYOu5OF7H8uFOwHv13QhCL-uS3bC2RbvY=s64","userId":"16007664423560638977"}},"outputId":"6f0cfa2d-a81a-4b19-a0d5-700b7be7766a","colab":{"base_uri":"https://localhost:8080/","height":438}},"source":["# Let's bring in our data processing libraries\n","import pandas as pd\n","import numpy as np\n","\n","# And look at some census data from the US\n","df = pd.read_csv('/content/drive/My Drive/Applied Data Science in Python/datasets/census.csv')\n","df.head()"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6bf3517a7c9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# And look at some census data from the US\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Applied Data Science in Python/datasets/census.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Applied Data Science in Python/datasets/census.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"QIPttxcS1TRE"},"source":["##Method Chaining\n","\n","The first of the pandas idioms I would like to talk about is **method chaining**. The general idea behind method chaining is that every method on an object returns a reference to that object. The beauty of this is that you can condense many different operations on a DataFrame into one line or at least one statement of code. This greatly increases the readability of the code, and is very common amongst Python programmers.\n","\n","For example, let's assume that I want to:\n","1. extract only county-level data, i.e. data which has a summary level of 50\n","2. set the state and city names as a multiple index, \n","3. rename a column too, just to make it a bit more readable\n","\n","Here's the \"pandorable\" way to write code with method chaining...\n"]},{"cell_type":"code","metadata":{"id":"OAWSCnI-X2T-"},"source":["(df[df['SUMLEV']==50]\n","  .set_index(['STNAME','CTYNAME'])\n","  .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nJRJzR5o8KUe"},"source":["Let's walk through this...\n","\n","First, we use the indexing operator to pass in a boolean mask which will only return the rows where the SUMLEV is equal to 50. This indicates in our source data that the data is summarized at the county level. \n","\n","We take the resulting dataframe returned, and set its index to the state name followed by the county name. Finally, we can rename a column to make it more readable. \n","\n","Note that instead of writing this all on one line, as I could have done, I began the statement with a parenthesis, which tells python I'm going to span the statement over multiple lines for readability."]},{"cell_type":"markdown","metadata":{"id":"YyTfb9xpf9XT"},"source":["Below is the more traditional, non-pandorable way, of writing this. There's nothing wrong with this code in the functional sense, you might even be able to understand it better as a new person to the language. It's just not as pandorable as the first example."]},{"cell_type":"code","metadata":{"id":"Ju4s0SyXX2T-"},"source":["# First create a new dataframe from the original\n","df = df[df['SUMLEV']==50]\n","# Update the dataframe to have a new index, we use inplace=True to do this in place\n","df.set_index(['STNAME','CTYNAME'], inplace=True)\n","# Set the column names\n","df.rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tl_ce1lRg2b4"},"source":["You'll see lots of examples on stack overflow and in documentation of people using method chaining in their pandas. As such, I think being able to read and understand the syntax is really worth your time."]},{"cell_type":"markdown","metadata":{"id":"NsHsfP5dP1MT"},"source":["##Apply Function\n","\n","Here's another pandas idiom. As we've learned in a previous lecture, Python has a wonderful function called `map()`, which is sort of a basis for functional programming in the language. When you want to use `map()` in Python, you pass it some function you want called, and some iterable, like a list, that you want the function to be applied to. The results are that the function is called against each item in the list, and there's a resulting list of all of the evaluations of that function.\n","\n","Pandas has a similar function called [`apply()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html), that I use often when wanting to map across all of the rows in a DataFrame. \n","\n","\n","\n","As an example, let's take a look at our census DataFrame. In this DataFrame, we have six columns for population estimates, each corresponding to a different year. Let's assume we want to create some new columns where we look at the minimum or maximum population estimates over the years for each county / state. The `apply()` function provides an easy way to do this.\n","\n","First, we need to write a function which *takes a row* from our DataFrame, finds a minimum and maximum values, and returns a new row of data.  We'll call this function `min_max`. We can do this by creating some small slice of a row, projecting the population columns, then use the NumPy `min` and `max` functions, and create a new series with label values representing the new values we want to apply.\n"]},{"cell_type":"code","metadata":{"id":"b6hZoNRbX2UA"},"source":["def min_max(row):\n","    data = row[['POPESTIMATE2010',\n","                'POPESTIMATE2011',\n","                'POPESTIMATE2012',\n","                'POPESTIMATE2013',\n","                'POPESTIMATE2014',\n","                'POPESTIMATE2015']] \n","    # The series here is the newly generated row\n","    return pd.Series({'min': np.min(data), 'max': np.max(data)})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1mHI4GqPrrC"},"source":["Now, we just need to call `apply()` on the DataFrame.\n","\n","[`apply()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html) takes the function and the axis on which to operate as parameters. Here, we have to be a bit careful, we've talked about axis zero being the rows of the DataFrame in the past, but this parameter expects you to pass along *the index* to use. So, to apply across all rows, you need to use the columns as the index, and thus pass `axis` equal to `'columns'`."]},{"cell_type":"code","metadata":{"id":"wKOqn-RmX2UB"},"source":["df.apply(min_max, axis='columns')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zq1RDWcVzCGL"},"source":["Of course there's no need to limit yourself to returning a new series object. If you're doing this as part of data cleaning, you're likely to find yourself wanting to add the new data to the existing DataFrame. In that case, you can just take the row values and add them as new columns indicating the max and min scores. This is a regular part of the data processing, inferring new data and building descriptive statistics, and is often used heavily with the merging of DataFrames.\n","\n","Let's adjust the code above, and produce a revised version of the function `min_max` that returns the original dataframe with two new columns labeled `max` and `min`.\n"]},{"cell_type":"code","metadata":{"id":"wQHswQgIX2UC"},"source":["def min_max(row):\n","    data = row[['POPESTIMATE2010',\n","                'POPESTIMATE2011',\n","                'POPESTIMATE2012',\n","                'POPESTIMATE2013',\n","                'POPESTIMATE2014',\n","                'POPESTIMATE2015']]\n","    # Create a new column for max\n","    row['max'] = np.max(data)\n","    # Create a new column for min\n","    row['min'] = np.min(data)\n","    #return the adjusted row\n","    return row\n","# Now just apply the function across the dataframe\n","df.apply(min_max, axis='columns')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32QB32785QZR"},"source":["`apply()` is an extremely important tool in your toolkit. However, you rarely use `apply()` with large function definitions, like we did. Instead, you typically see it used with `lambda`.\n","\n","Let's see how we can convert the above code to one line of code using method chaining, as well as `lambda` in the `apply()` function. "]},{"cell_type":"code","metadata":{"id":"ROYpWyaOX2UC"},"source":["df[['POPESTIMATE2010', 'POPESTIMATE2011', 'POPESTIMATE2012', \n","    'POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015']].apply(lambda x: np.max(x), axis=1).head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xcL6o4C69Wj4"},"source":["Remember, `lambda` is basically an unamed function. In this case it takes a single parameter, x, and returns a single value: the maximum over all columns associated with row x.\n","\n","How would we adjust the above line of code to make it return a new column in the same dataframe, labeled \"max\"?"]},{"cell_type":"code","metadata":{"id":"ijh6NHwm9wNU"},"source":["df['max']= df[['POPESTIMATE2010', 'POPESTIMATE2011', 'POPESTIMATE2012',\n","               'POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015']].apply(lambda x: np.max(x), axis=1)\n","\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWW1bp-qA8ca"},"source":["The beauty of the `apply()` function is that it allows flexibility in doing whatever manipulation you desire, as the function you pass into `apply()` can be customized however you want. \n","\n","Let's say we want to divide the states into four categories: Northeast, Midwest, South, and West. We can write a customized function that returns the region based on the state."]},{"cell_type":"code","metadata":{"id":"BiCruNONX2UD"},"source":["def get_state_region(x):\n","    northeast = ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', \n","                 'Rhode Island','Vermont','New York','New Jersey','Pennsylvania']\n","    midwest = ['Illinois','Indiana','Michigan','Ohio','Wisconsin','Iowa',\n","               'Kansas','Minnesota','Missouri','Nebraska','North Dakota',\n","               'South Dakota']\n","    south = ['Delaware','Florida','Georgia','Maryland','North Carolina',\n","             'South Carolina','Virginia','District of Columbia','West Virginia',\n","             'Alabama','Kentucky','Mississippi','Tennessee','Arkansas',\n","             'Louisiana','Oklahoma','Texas']\n","    west = ['Arizona','Colorado','Idaho','Montana','Nevada','New Mexico','Utah',\n","            'Wyoming','Alaska','California','Hawaii','Oregon','Washington']\n","    \n","    if x in northeast:\n","        return \"Northeast\"\n","    elif x in midwest:\n","        return \"Midwest\"\n","    elif x in south:\n","        return \"South\"\n","    else:\n","        return \"West\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJP_ASnZBVCA"},"source":["Now that we have the customized function, let's say we want to create a new column called Region, showing the state's region. We can use the customized function and the `apply()` function to do so. The customized function is supposed to work on the state name column STNAME. So we will set the apply function on the state name column and pass the customized function into the apply function."]},{"cell_type":"code","metadata":{"id":"uYNkwl48X2UD"},"source":["df['state_region'] = df['STNAME'].apply(lambda x: get_state_region(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wnjVYx4MX2UE"},"source":["# Now let's see the results\n","df[['STNAME','state_region']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I2XGl5NdDcwI"},"source":["##GroupBy Function"]},{"cell_type":"markdown","metadata":{"id":"NlmPrqVGDC39"},"source":["Sometimes we want to select data based on groups and understand *aggregated* data on a group level. We have seen that even though Pandas allows us to iterate over every row in a dataframe, it is generally very slow to do so. \n","\n","Fortunately Pandas has a `groupby()` function that speeds up such tasks. The idea behind the `groupby()` function is that it takes some dataframe, splits it into chunks based on some key values, applies computation on those  chunks, then combines the results back together into another dataframe. In pandas, this is refered to as the ***split-apply-combine* pattern.**\n","\n","![split-apply-combine](https://drive.google.com/uc?id=1IAqOC1pco14exirexw9YjgXxFe3vAPvq)"]},{"cell_type":"markdown","metadata":{"id":"8Pl1MeFxDC4C"},"source":["### Splitting\n","\n","In the first example for `groupby()` I will re-use the census date. "]},{"cell_type":"code","metadata":{"id":"kAonA9zeDC4D"},"source":["# Let's look at some US census data\n","df = pd.read_csv('/content/drive/My Drive/Applied Data Science in Python/datasets/census.csv')\n","# And exclude state level summarizations, which have sum level value of 40\n","df = df[df['SUMLEV']==50]\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QOttoGIrGRon"},"source":["Let's assume we want to calculate that average population of the counties within each state using the `CENSUS2010POP` column. \n","\n","In order to do so, one impulsive way to do that is to generate a list of the unique states, then iterate over all the states, and for each state we produce a dataframe and calculate the average.\n","\n","And to see how well this method is, let's run such the code 3 times and time it. For this we'll use the cell magic function we used before, `%%timeit`."]},{"cell_type":"code","metadata":{"id":"gnpTP95dDC4E"},"source":["%%timeit -n 3\n","\n","for state in df['STNAME'].unique():\n","    # We'll just calculate the average using numpy for this particular state\n","    avg = np.average(df[df['STNAME']==state]['CENSUS2010POP'])\n","    # And we'll print it to the screen\n","    print('Counties in state ' + state + \n","          ' have an average population of ' + str(avg))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GVYlTYLJI0dB"},"source":["If you scroll down to the bottom of that output you can see it takes a fair bit of time to finish.\n","\n","Now let's try doing the same thing using [`groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html).\n","\n","The `groupby()` **returns a GroupBy object**. When iterating over a GroupBy object, it returns a tuple for every \"group\":\n","1. the value of the key we were trying to group by, in this case a specific state name,\n","2. the projected dataframe for that group"]},{"cell_type":"code","metadata":{"id":"JvrhXbqlDC4F"},"source":["%%timeit -n 3\n","# For this method, we start by telling pandas we're interested in grouping by state name, this is the \"split\"\n","for group, frame in df.groupby('STNAME'):\n","    # Now we include our logic in the \"apply\" step, which is to calculate an average of the census2010pop\n","    avg = np.average(frame['CENSUS2010POP'])\n","    # And print the results\n","    print('Counties in state ' + group + \n","          ' have an average population of ' + str(avg))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mm_VnA3tUbRr"},"source":["Look at the difference in speed! GroupBy improved the performace by roughly two factors!"]},{"cell_type":"markdown","metadata":{"id":"JseKoPXo4dJq"},"source":["Now, most of the time, you'll use `groupby()` on one or more columns. However, it is good to know that you can also provide a function to `groupby()` and use that to segment your data.\n","\n","Let's look at an example. Say you have a big batch job with lots of processing and you want to work on only a third or so of the states at a given time. As a result, you want to create some function which allocates each row into one of 3 groups based on the first letter of the state name. Then we can tell `groupby()` to use this function to \"split\" up our data frame. It's important to note that in order to do this you need to set the index of the dataframe to be the column that you want to group by first (as stated in the [API reference](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) : \"If `by` is a function, it’s called on each value of the object’s index.\")\n","\n","We'll create some new function called `set_group` and if the first letter of the parameter is from A to L we'll return \"A to L\". If it's a from M to P we'll return \"M to P\", otherwise we'll return \"Q to Z\". Then we'll pass this function to the data frame."]},{"cell_type":"code","metadata":{"id":"4j2xMuDrDC4G"},"source":["df = df.set_index('STNAME')\n","\n","def set_group(index):\n","    if index[0]<'M':\n","        return \"A to L\"\n","    if index[0]<'Q':\n","        return \"M to P\"\n","    return \"Q to Z\"\n","\n","# The dataframe is supposed to be grouped by according to the batch\n","# We will then loop through each batch group\n","for group, frame in df.groupby(set_group):\n","    print('There are ' + str(len(frame)) + ' records in group ' + group + ' for processing.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CbfbIxI1CkpE"},"source":["Let's take one more look at an example of how we might group data. In this example, we use a dataset of housing from airbnb. In this dataset, there are two columns of interest, one is the `cancellation_policy` and the other is the `review_scores_value`."]},{"cell_type":"code","metadata":{"id":"OH7PjNlODC4H"},"source":["import pandas as pd\n","import numpy as np\n","\n","df=pd.read_csv(\"/content/drive/My Drive/Applied Data Science in Python/datasets/listings.csv\")\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JvDOlfUlH_7I"},"source":["So, how would I group by both of these columns? One approach might be to promote them to a multiindex and just call `groupby()`."]},{"cell_type":"code","metadata":{"id":"M4-oQhnXDC4H"},"source":["df=df.set_index([\"cancellation_policy\",\"review_scores_value\"])\n","\n","# When we have a multiindex we need to pass in the levels we are interested in grouping by\n","for group, frame in df.groupby(level=(0,1)):\n","    print(group)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GYLOIbfRMGTg"},"source":["This seems to work ok. But what if we wanted to group by the cancelation policy and review scores, but separate out all the 10's from those under ten? In this case, we could use a function to manage the groupings"]},{"cell_type":"code","metadata":{"id":"M2MI_nq5DC4I"},"source":["def grouping_fun(index_):\n","    # Check the \"review_scores_value\" portion of the index.\n","    # index_ is in the format of (cancellation_policy,review_scores_value)\n","    if index_[1] == 10.0:\n","        return (index_[0],\"10.0\")\n","    else:\n","        return (index_[0],\"not 10.0\")\n","\n","for group, frame in df.groupby(by=grouping_fun):\n","    print(group)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ww384-sNDC4J"},"source":["### Applying"]},{"cell_type":"markdown","metadata":{"id":"9-fMaZwJNbk6"},"source":["To this point we have applied very simple processing to our data after splitting, which is simply outputting some print statements to demonstrate how the splitting works. The pandas developers have three broad categories of data processing to happen during the apply step: \n","1. Aggregation of group data,\n","2. Transformation of group data\n","3. Filtration of group data"]},{"cell_type":"markdown","metadata":{"id":"JLDypTQJDC4J"},"source":["#### Aggregation"]},{"cell_type":"markdown","metadata":{"id":"0hWbUozTOEra"},"source":["The most straight forward \"apply\" step is the aggregation of data, using the method [`agg()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html) on the groupby object. Thus far we have only iterated through the GroupBy object, unpacking it into a label (the group name) and a dataframe. But with `agg()` we can pass in a dictionary of the columns we are interested in aggregating along with the function we are looking to apply to aggregate."]},{"cell_type":"code","metadata":{"id":"vwtjGOw7DC4J"},"source":["# Let's reset the index for our airbnb data\n","# which puts \"cancellation_policy\" and \"review_scores_value\" back as columns\n","df=df.reset_index()\n","\n","# Now lets group by the cancellation policy and find the average review_scores_value by group\n","df.groupby(\"cancellation_policy\").agg({\"review_scores_value\":np.average})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64cK1lrxQUM1"},"source":["So that didn't work, and returned a bunch of `NaN`s. The issue is in the function that we sent to aggregate. `np.average()` does not ignore `NaN`s! However, there is a function we can use for this `np.nanmean()`."]},{"cell_type":"code","metadata":{"id":"YbV8fj16DC4K"},"source":["df.groupby(\"cancellation_policy\").agg({\"review_scores_value\":np.nanmean})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_v8dzT-0TVGa"},"source":["We can also extend this dictionary to aggregate multiple functions and/or multiple columns.\n"]},{"cell_type":"code","metadata":{"id":"PARN1ZWoDC4K"},"source":["df.groupby(\"cancellation_policy\").agg({\"review_scores_value\":(np.nanmean,np.nanstd),\n","                                      \"reviews_per_month\":np.nanmean})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WHyL8NxDTpBX"},"source":["Take a moment to make sure you understand the previous cell, since it's somewhat complex. First we're doing a `groupby()` on the dataframe object by the column `\"cancellation_policy\"`. This creates a new GroupBy object. Then we are invoking the `agg()` function on that object. The `agg()` function is going to apply the one or more functions we specify to the group dataframes and *return a single row per dataframe/group*. When we called this function we sent it two dictionary entries, each with the key indicating which column we wanted functions applied to. \n","\n","For the first column we actually supplied a tuple of two functions. Note that these are not function invocations, like `np.nanmean()`, or function names, like `\"nanmean\"` they are references to functions which will return single values. The groupby object will recognize the tuple and call each function in order on the same column. The results will be in a heirarchical index, but since they are columns they don't show as an index per se. Finally, we indicated another column and a single function we wanted to run."]},{"cell_type":"markdown","metadata":{"id":"1-BLT5GuDC4L"},"source":["#### Transformation\n","\n","Transformation is different from aggregation. **The `agg()` function returns a single value per column, and one row per group.** On the other hand, the `tranform()` function returns an object that is the same size as the group. Essentially, **`tranform()` broadcasts the function you supply over the grouped dataframe, returning a new dataframe**. This makes combining data later easy.\n","\n","For instance, suppose we want to include the average rating values in a given group by cancellation policy, but preserve the dataframe shape so that we could generate a difference between an individual observation and the sum.\n"]},{"cell_type":"code","metadata":{"id":"z46ARBKODC4L"},"source":["# First, lets define just some subset of columns we are interested in\n","cols = ['cancellation_policy','review_scores_value']\n","\n","# Now lets transform it, and store it in its own dataframe\n","transform_df = df[cols].groupby('cancellation_policy').transform(np.nanmean)\n","transform_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpPbkGBIYfTb"},"source":["As you can see that the index here is actually the same as the original dataframe. So to include these new columns to the original dataframe, we can just merge this in. However, before we do that, let's rename the column in the transformed version."]},{"cell_type":"code","metadata":{"id":"j2BI-xPnDC4L"},"source":["transform_df.rename({'review_scores_value':'mean_review_scores'}, axis='columns', inplace=True)\n","df=pd.merge(df,transform_df, left_index=True, right_index=True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttrHj2gyZ-z-"},"source":["Great, we can see that our new column is in place, the `mean_review_scores`. So now we could create, for instance, the difference between a given row and it's group (the cancellation policy) means."]},{"cell_type":"code","metadata":{"id":"kkKtocdpDC4M"},"source":["df['mean_diff']=df['review_scores_value']-df['mean_review_scores']\n","df['mean_diff'].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVcJHmTbDC4M"},"source":["#### Filtering"]},{"cell_type":"markdown","metadata":{"id":"I4NiSG5Gauw2"},"source":["The GroupBy object has built-in support for filtering groups as well. It's often that you'll want to group by some feature, then make some transformation to the groups, then drop certain groups as part of your cleaning routines. The **`filter()` function takes in a function which it applies to each group dataframe and returns either a `True` or a `False`**, depending upon whether that group should be included in the results."]},{"cell_type":"code","metadata":{"id":"mPSa6F1tDC4M"},"source":["#Let's first look at the average \n","print (df[['cancellation_policy','review_scores_value']].groupby('cancellation_policy').mean())\n","# For instance, if we only want those groups which have a mean rating above 9.2 included in our results\n","df[['cancellation_policy','review_scores_value']].groupby('cancellation_policy').filter(lambda x: np.nanmean(x['review_scores_value'])>9.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIw1w7DT-I_K"},"source":["Notice that the results are still indexed, but that any of the results which were in a group with a mean review score of less than or equal to 9.2 (i.e. `strict` and `super_strict_30`) were not copied over."]},{"cell_type":"markdown","metadata":{"id":"RtZ8Zc7WDC4N"},"source":["#### Applying"]},{"cell_type":"markdown","metadata":{"id":"BohPvkLJ--P9"},"source":["By far the most common operation python programmers invoke on groupby objects is the [`apply()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.apply.html) function. This allows you to apply an arbitrary function to each group, and stitch the results back into a single dataframe while the index is preserved."]},{"cell_type":"code","metadata":{"id":"AbEtAAebDC4N"},"source":["# Lets look at an example using our airbnb data, I'm going to get a clean copy of the dataframe\n","df=pd.read_csv(\"/content/drive/My Drive/Applied Data Science in Python/datasets/listings.csv\")\n","# And lets just include some of the columns we were interested in previously\n","df=df[['cancellation_policy','review_scores_value']]\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09gRXiXTAe22"},"source":["In previous work, we wanted to find the average review score of a listing and its deviation from the group mean. This was a two step process, first we used `transform()` on the groupby object and then we had to broadcast to create a new column. With `apply()` we could wrap this logic in one place."]},{"cell_type":"code","metadata":{"id":"_nmrCC1eDC4N"},"source":["def calc_mean_review_scores(group):\n","    # group is a dataframe just of whatever we have grouped by, e.g. cancellation policy, so we can treat\n","    # this as the complete dataframe\n","    avg=np.nanmean(group[\"review_scores_value\"])\n","    # now broadcast our formula and create a new column\n","    group[\"review_scores_mean\"]=np.abs(avg-group[\"review_scores_value\"])\n","    return group\n","\n","# Now just apply this to the groups\n","df.groupby('cancellation_policy').apply(calc_mean_review_scores).head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TUxUcPtuAs2u"},"source":["Using `apply()` can be slower than using some of the specialized functions, especially `agg()`. However, if your dataframes are not huge, it's a solid general purpose approach.\n","\n","Finally,  pandas developers have provided the GroupBy object with [built-in functions for the most commonly used computations](https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html), and it is worth investing some time to look at these functions and read them."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"lsnx3qMaDC4N"},"source":["Groupby is a powerful and commonly used tool for data cleaning and data analysis. Once you have grouped the data by some category you have a dataframe of just those values and you can conduct aggregated analysis on the segments that you are interested in. The `groupby()` function follows a split-apply-combine approach - first the data is split into subgroups, then you can apply some transformation, filtering, or aggregation, then the results are combined automatically by pandas for us."]}]}