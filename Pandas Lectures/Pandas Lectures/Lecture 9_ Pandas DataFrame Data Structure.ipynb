{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Lecture 9_ Pandas DataFrame Data Structure.ipynb","provenance":[{"file_id":"1WyobCOQ9XdZFaNVKPq_Ro7CF5O9Df51p","timestamp":1608126612618}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"N6eOclEn0Jee"},"source":["-------\n","#Pandas DataFrame Data Structure\n","-------\n","\n","The **DataFrame** data structure is the heart of the Panda's library. It's the primary object that you'll be working with in data analysis and cleaning tasks.\n","\n","The DataFrame is conceptually a two-dimensional series object, with an index and multiple columns of content, where each column has a label. In fact, the distinction between a column and a row is really only a conceptual distinction. And you can think of the DataFrame itself as simply a two-axes labeled array."]},{"cell_type":"markdown","metadata":{"id":"LIdFvkHfJgeP"},"source":["##Creating DataFrames"]},{"cell_type":"code","metadata":{"id":"crvRUEXI0Jel","executionInfo":{"status":"ok","timestamp":1632733184100,"user_tz":-240,"elapsed":550,"user":{"displayName":"Bedoor AlShebli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidDz9yYOu5OF7H8uFOwHv13QhCL-uS3bC2RbvY=s64","userId":"16007664423560638977"}}},"source":["# Let's start by importing our pandas library\n","import pandas as pd"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OvaEfQfhDidK"},"source":["Let's start with an example. Let's create three school records for students and their class grades. I'll create each as a series which has a student name, the class name, and the score. "]},{"cell_type":"code","metadata":{"id":"ujBzRfrp0Jen"},"source":["record1 = pd.Series({'Name': 'Alice',\n","                     'Class': 'Physics',\n","                     'Score': 85})\n","record2 = pd.Series({'Name': 'Jack',\n","                     'Class': 'Chemistry',\n","                     'Score': 82})\n","record3 = pd.Series({'Name': 'Helen',\n","                     'Class': 'Biology',\n","                     'Score': 90})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RRYA7i7fEOAl"},"source":["Like a Series, the DataFrame object is also indexed. Here we'll use a group of series, where each series represents a row of data. Just like the Series function, we can pass in our individual item in an array, and we can pass in our index values as a second arguments.\n","\n","And just like the Series we can use the `head()` function to see the first several rows of the dataframe, including indices from both axes, and we can use this to verify the columns and the rows"]},{"cell_type":"code","metadata":{"id":"PEuo65oZ0Jen"},"source":["df = pd.DataFrame([record1, record2, record3],\n","                  index=['school1', 'school2', 'school1'])\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8k-5Od77GLQy"},"source":["You'll notice here that Jupyter creates a nice bit of HTML to render the results of the dataframe. So we have the index, which is the leftmost column and is the school name, and then we have the rows of data, where each row has a column header which was given in our initial record dictionaries."]},{"cell_type":"markdown","metadata":{"id":"I43U2Ky3JpTq"},"source":["An alternative method is that you could use a list of dictionaries, where each dictionary represents a row of data. We then pass this list of dictionaries into the `pd.DataFrame()` function."]},{"cell_type":"code","metadata":{"id":"t2llR4k40Jep"},"source":["students = [{'Name': 'Alice',\n","              'Class': 'Physics',\n","              'Score': 85},\n","            {'Name': 'Jack',\n","             'Class': 'Chemistry',\n","             'Score': 82},\n","            {'Name': 'Helen',\n","             'Class': 'Biology',\n","             'Score': 90}]\n","\n","df = pd.DataFrame(students, index=['school1', 'school2', 'school1'])\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"szP9yYo24eHe"},"source":["##Accessing Data in DataFrames"]},{"cell_type":"markdown","metadata":{"id":"BEaxEY0GKGOr"},"source":["Similar to the series, we can extract data using the `.iloc` and `.loc` attributes. Because the DataFrame is two-dimensional, passing a single value to the loc indexing operator will return the series if there's only one row to return.\n","\n","For instance, if we wanted to select data associated with school2, we would just query the `.loc` attribute with one parameter."]},{"cell_type":"code","metadata":{"id":"ZKT4i5TA0Jeq"},"source":["df.loc['school2']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INrH7fJBKZgO"},"source":["You'll note that the name of the series is returned as the index value, while the column name is included in the output.\n","\n","We can check the data type of the return using the python `type` function."]},{"cell_type":"code","metadata":{"id":"MvAQCcMW0Jeq"},"source":["type(df.loc['school2'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"reC7dO4dKv8E"},"source":["It's important to remember that the indices and column names along either axes horizontal or vertical, could be *non-unique*. \n","\n","In this example, we see two records for school1 as different rows. If we use a single value with the DataFrame `.loc` attribute, multiple rows of the DataFrame will return, not as a new series, but as a new DataFrame."]},{"cell_type":"code","metadata":{"id":"q83nGfac0Jer"},"source":["df.loc['school1']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PNmhWfFaLOi2"},"source":["And we can see the type is now different too..."]},{"cell_type":"code","metadata":{"id":"xVLLDbo70Jer"},"source":["type(df.loc['school1'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kk7MyeeyL0cw"},"source":["One of the powers of the Panda's DataFrame is that you can quickly select data based on multiple axes.\n","\n","For instance, if you wanted to just list the student names for school1, you would supply two parameters to `.loc`, one being the row index and the other being the column name."]},{"cell_type":"code","metadata":{"id":"yPTas5QO0Jes"},"source":["df.loc['school1', 'Name']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zbf2ZQDeMOGN"},"source":["Remember, just like the Series, the pandas developers have implemented this using the indexing operator and not as parameters to a function.\n","\n","What would we do if we just wanted to select a single column though? Well, there are a few mechanisms. Firstly, we could transpose the matrix. This pivots all of the rows into columns and all of the columns into rows, and is done with the `T` attribute."]},{"cell_type":"code","metadata":{"id":"tou1QwTC0Jes"},"source":["df.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-FbzquQVMY-q"},"source":["Then we can call `.loc` on the transpose to get the student names only"]},{"cell_type":"code","metadata":{"id":"2Hv6J-cI0Jet"},"source":["df.T.loc['Name']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9P22unyrMecl"},"source":["However, since `iloc` and `loc` are used for row selection, Pandas reserves the indexing operator directly on the DataFrame for column selection. In a Pandas DataFrame, columns always have a name. So this selection is always label based, and is not as confusing as it was when using the square bracket operator on the series objects. For those familiar with relational databases, this operator is analogous to column projection."]},{"cell_type":"code","metadata":{"id":"R_pWoKaX0Jeu"},"source":["df['Name']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jU65jRcMyXB"},"source":["In practice, this works really well since you're often trying to add or drop new columns. However, this also means that you get a key error if you try and use `.loc` with a column name."]},{"cell_type":"code","metadata":{"id":"VHHMgXTv0Jeu"},"source":["df.loc['Name']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_zQgyWsM7Ro"},"source":["Note too that the result of a single column projection is a Series object"]},{"cell_type":"code","metadata":{"id":"HrTcDkSu0Jev"},"source":["type(df['Name'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_lpGZbZNA_O"},"source":["Since the result of using the indexing operator is either a DataFrame or Series, you can chain operations together. For instance, we can select all of the rows which related to school1 using `.loc`, then project the name column from just those rows."]},{"cell_type":"code","metadata":{"id":"Og-gbk5e0Jev"},"source":["df.loc['school1']['Name']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KmuAR16XNQRs"},"source":["If you get confused, use type to check the responses from resulting operations"]},{"cell_type":"code","metadata":{"id":"C35-0Q9i0Jev"},"source":["print(type(df.loc['school1'])) #should be a DataFrame\n","print(type(df.loc['school1']['Name'])) #should be a Series"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWokNrntNgXp"},"source":["*Chaining*, i.e. indexing on the return type of another index, can come with some costs and is best avoided if you can use another approach. In particular, chaining tends to cause Pandas to return a copy of the DataFrame instead of a view on the DataFrame.\n","\n","For selecting data, this is not a big deal, though it might be slower than necessary. If you are changing data though, this is an important distinction and can be a source of error.\n","\n","Here's another approach. As we saw, `.loc` does row selection, and it can take two parameters, the row index and the list of column names. The `.loc` attribute also supports slicing.\n","\n","If we wanted to select all rows, we can use a colon to indicate a full slice from beginning to end. This is just like slicing characters in a list in python. Then we can add the column name as the second parameter as a string. If we wanted to include multiple columns, we could do so in a list. and Pandas will bring back only the columns we have asked for.\n","\n","Here's an example, where we ask for all the names and scores for all schools using the `.loc` operator."]},{"cell_type":"code","metadata":{"id":"7HU50S-y0Jex"},"source":["df.loc[:,['Name', 'Score']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7B1xa0uS26sE"},"source":["Take a look at that again. The colon means that we want to get all of the rows, and the list in the second argument position is the list of columns we want to get back.\n","\n","Another shortcut way to get the same results, if you want all rows, is to remove `.loc` attritubute and just index the list of columns."]},{"cell_type":"code","metadata":{"id":"wQ4Bn8E7nS9e"},"source":["df[['Name','Score']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D3ZiTqsg31Qr"},"source":["That's selecting and projecting data from a DataFrame based on row and column labels. The key concepts to remember are that the rows and columns are really just for our benefit. Underneath this is just a two axes labeled array, and transposing the columns is easy. Also, consider the issue of chaining carefully, and try to avoid it, as it can cause unpredictable results, where your intent was to obtain a view of the data, but instead Pandas returns to you a copy.\n","\n","The below provides a quick summary overview of accessing data in a DataFrame:\n","\n","![dataframe.png](https://drive.google.com/uc?id=1t83tb2TSBEojwRU6MWC-Xzp_7AcdXZFj)"]},{"cell_type":"markdown","metadata":{"id":"ghiBAV1X709P"},"source":["##Dropping Data in DataFrames\n","\n","Let's talk about dropping data. It's easy to delete data in Series and DataFrames, and we can use the `.drop()` function to do so. This function takes a single parameter, either the index or row label, to drop. \n","\n","Note though that the drop function doesn't change the DataFrame by default! Instead, the `.drop()` function returns a copy of the DataFrame with the given rows removed."]},{"cell_type":"code","metadata":{"id":"XSEdIoDj0Jey"},"source":["df.drop('school1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tN1NCjmD5qlQ"},"source":["But if we look at our original DataFrame we see the data is still intact."]},{"cell_type":"code","metadata":{"id":"ZGYIbqXO0Jey"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KBSzi2R5yoC"},"source":["Drop has two interesting optional parameters. The first is called `inplace`, when set to `True`, the DataFrame will be updated in place and no copy will be returned. The second parameter is `axis`,where we identify which Axis should be dropped. By default, this value is `0`, indicating the row axis. In order to drop a column, you need to change it to `1`."]},{"cell_type":"code","metadata":{"id":"T3W0LC520Jez"},"source":["# Now lets drop the Name column\n","df.drop(\"Name\", inplace=True, axis=1)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTY8_8pY7Eyb"},"source":["There is a second way to drop a column, and that's directly through the use of the indexing operator and the `del` keyword. This way of dropping data, however, takes immediate effect on the DataFrame and does not return a copy."]},{"cell_type":"code","metadata":{"id":"xmOQkEsF0Jez"},"source":["del df['Class']\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0qS_uYcY7lJK"},"source":["Finally, adding a new column to the DataFrame is as easy as assigning it to some value using the indexing operator. For instance, if we wanted to add a class ranking column with default value of `None`, we could do so by using the assignment operator after the square brackets. This broadcasts the default value to the new column immediately."]},{"cell_type":"code","metadata":{"id":"9eWeSaq90Je0"},"source":["df['ClassRanking'] = None\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHRbF73N8rSc"},"source":["##DataFrame Indexing and Loading"]},{"cell_type":"markdown","metadata":{"id":"tSo5XtLI8WGL"},"source":["Throughout the course, we'll be largely using smaller or moderate-sized datasets. A common workflow is to read the dataset, usually from some external file, then begin to clean and manipulate the dataset for analysis. So, let's demonstrate how you can load data from a comma separated file directly into a DataFrame.\n","\n","Let's talk about comma separated values (csv) files. You've undoubtedly used these - any spreadsheet software like excel or google sheets can save output in CSV format. It's pretty loose as a format, incredibly lightweight, and totally ubiquitous.\n","\n","Pandas mades it easy to turn a CSV into a DataFrame by just calling the `.read_csv()` function."]},{"cell_type":"code","metadata":{"id":"IGDK1iybBfN-"},"source":["# Again, to access the file, you need to mount the drive. \n","# If you are running jupyter notebook locally no need to do this step.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!ls /content/drive/My\\ Drive/Applied\\ Data\\ Science\\ in\\ Python/datasets/  # Running a line with a \"!\" in the start is identical to running a bash script"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PSr7YmY8WGT"},"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/drive/My Drive/Applied Data Science in Python/datasets/Admission_Predict.csv')\n","\n","# And let's look at the first 5 rows\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MnGPVKGVAtru"},"source":["We see from the output that there is a list of columns, and the column identifiers are listed as strings on the first line of the file. Then we have rows of data, all columns values are values separated by commas in the original file. \n","\n","So now we have all the values in the file organized in a \"table-like\" format, aka the DataFrame. Notice, however, that by default the index starts with 0 while the students' serial number starts from 1. If you take a look at the CSV output, that the index isn't there and you'll deduce that pandas has created its own new index. So, we can either keep it as it is, or instead, we can set the serial no. as the index if we want to by using the `index_col` parameter."]},{"cell_type":"code","metadata":{"id":"WuFKSJRy8WGT"},"source":["df = pd.read_csv('/content/drive/My Drive/Applied Data Science in Python/datasets/Admission_Predict.csv', index_col=\"Serial No.\")\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8EjsVfpEyV6"},"source":["Notice also that we have two columns \"SOP\" and \"LOR\", that many probably do not know what they mean. So let's change our column names to make it more clear. In Pandas, we can use the `.rename()` function, which takes a\n","parameter called `columns` that takes a dictionary where the keys are the old column names and the values are their corresponding new column names."]},{"cell_type":"code","metadata":{"id":"ea-uIu5u8WGT"},"source":["new_df=df.rename(columns={'SOP': 'Statement of Purpose','LOR': 'Letter of Recommendation'})\n","new_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPe7vbDQGCv0"},"source":["From the output, we can see that only \"SOP\" is changed but not \"LOR\". Why is that? \n","\n","Let's investigate this a bit. First we need to make sure we got all the column names correct. We can use the `.columns` attribute of DataFrame to get a list."]},{"cell_type":"code","metadata":{"id":"mr1LE2c28WGU"},"source":["new_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3lzgIxAdGWhP"},"source":["If we look at the output closely, we can see that there is actually a space right after \"LOR\" and a space right after \"Chance of Admit\". Sneaky, huh? So this is why our rename dictionary did not work for \"LOR\", because the key we used was just three characters, instead of \"LOR \".\n","\n","There are a couple of ways we could address this. One way would be to change the column name by including the space in the name."]},{"cell_type":"code","metadata":{"id":"Z_xQyI438WGU"},"source":["new_df=new_df.rename(columns={'LOR ': 'Letter of Recommendation'})\n","new_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jYcnMIVkGtjk"},"source":["So that works well, but it's a bit fragile. What if that was a tab instead of a space? Or two spaces?\n","\n","Another way is to create some function that does the cleaning and then tell `.rename()` to apply that function across all of the data. Python does that for us using a handy string function, called `strip()`, that strips white space. We can pass this function in to rename via the `mapper` parameter, which maps any function into the values of the axis indicated. Finally, we indicate whether the axis should be `\"columns\"` or `\"index\"` (row labels)."]},{"cell_type":"code","metadata":{"id":"J5f6D-pN8WGU"},"source":["new_df=new_df.rename(mapper=str.strip, axis=\"columns\")\n","# Let's take a look at results\n","print(new_df.columns)\n","new_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uuzHCOzUHE3a"},"source":["Now we've got it - both \"SOP\" and \"LOR\" have been renamed and \"Chance of Admit\" has been trimmed up. \n","\n","Remember though that the rename function isn't modifying the DataFrame. In this case, df is the same as it always was, there's just a copy in `new_df` with the changed names."]},{"cell_type":"code","metadata":{"id":"9dovnZPZ8WGV"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"72twSuqGH2As"},"source":["We can also use the `df.columns` attribute by assigning to it a list of column names which will directly rename the columns. This will directly modify the original dataframe and is very efficient especially when you have a lot of columns and you only want to change a few. This technique is also not affected by subtle errors in the column names, a problem that we just encountered, it will just overwrite the existing names in the locations specified. With a list, you can use the list index to change a certain value or use list comprehension to change all of the values."]},{"cell_type":"code","metadata":{"id":"wrp9vcua8WGV"},"source":["# As an example, lets change all of the column names to lower case. First we need to get our list\n","cols = list(df.columns)\n","# Then a little list comprehenshion\n","cols = [x.lower().strip() for x in cols]\n","# Change individual column names\n","cols[3]=\"statement of purpose\"\n","# Then we just overwrite what is already in the .columns attribute\n","df.columns=cols\n","# And take a look at our results\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"juJAEuPv8WGV"},"source":["So far we've learned how to import a CSV file into a pandas DataFrame object, and how to do some basic data cleaning to the column names. The CSV file import mechanisms in pandas have [lots of different options](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), and you really need to learn these in order to be proficient at data manipulation. Once you have set up the format and shape of a DataFrame, you have a solid start to further actions such as conducting data analysis and modeling.\n","\n","Now, there are other data sources you can load directly into dataframes as well, including HTML web pages, databases, and other file formats. However, the CSV file format is by far the most common data format you'll run into, and an important one to know how to manipulate in pandas."]},{"cell_type":"markdown","metadata":{"id":"sw5rHPCuMLQR"},"source":["##Querying DataFrames using Boolean Masks"]},{"cell_type":"markdown","metadata":{"id":"tu_SI6egMUJa"},"source":["Let's talk about querying DataFrames. The first step in the process is to understand **Boolean masking**. Boolean masking is the heart of fast and efficient querying in NumPy and Pandas, and its analogous to bit masking used in other areas of computational science.\n","\n","A boolean mask is an array which can be of one dimension like a Series, or two dimensions like a DataFrame, where each of the values in the array are either `True` or `False`. This array is essentially overlaid on top of the data structure that we're querying. Any cell aligned with the `True` value will be admitted into our final result, and any cell aligned with a `False` value will not.\n","\n","Here' an illustration of how a boolean mask would work in a DataFrame:\n","\n","![boolean mask](https://drive.google.com/uc?id=1Of1IXHiESQssq6ZQTFa_sYn5brDuyjz4)"]},{"cell_type":"markdown","metadata":{"id":"0QwDHNBLW68d"},"source":["Let's look at our graduate admission dataset again."]},{"cell_type":"code","metadata":{"id":"P7IqHnchMUJk"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zfDCo65QWbRl"},"source":["Boolean masks are created by applying operators directly to the pandas Series or DataFrame objects. For instance, in our graduate admission dataset, we might be interested in seeing only those students that have a chance higher than 0.7\n","\n","To build a Boolean mask for this query, we want to project the \"chance of admit\" column using the indexing operator and apply the greater than operator with a comparison value of 0.7. This is essentially broadcasting a comparison operator, greater than, with the results being returned as a Boolean Series. The resultant Series is indexed where the value of each cell is either `True` or `False` depending on whether a student has a chance of admit higher than 0.7"]},{"cell_type":"code","metadata":{"id":"9xAfhA8iMUJm"},"source":["admit_mask=df['chance of admit'] > 0.7\n","admit_mask.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QtNCP-imYkce"},"source":["This is pretty fundamental, so take a moment to look at this. The result of broadcasting a comparison operator is a Boolean mask; i.e. `True` or `False` values depending upon the results of the comparison. Underneath, pandas is applying the comparison operator you specified through vectorization (so efficiently and in parallel) to all of the values in the array you specified which, in this case, is the \"chance of admit\" column. In this case, the result is a series, since we are only applying the operator on one column."]},{"cell_type":"markdown","metadata":{"id":"nw_0-Ayz2CCj"},"source":["So, what do you do with the boolean mask once you have formed it? Well, you can just lay it on top of the data to \"hide\" the data you don't want, which is represented by all of the `False` values. This can be done using the `.where()` function on the original DataFrame."]},{"cell_type":"code","metadata":{"id":"CG80T1YSMUJn"},"source":["df.where(admit_mask).head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-bWIdjYN2I89"},"source":["We see that the resulting DataFrame keeps the original indexed values, and only data which met the condition was retained. All of the rows which did not meet the condition have `NaN` data instead, but these rows were not dropped from our dataset. \n","\n","Now, if we don't want the `NaN` data, the next step would be to use the `dropna()` function"]},{"cell_type":"code","metadata":{"id":"MOzcgg24MUJo"},"source":["df.where(admit_mask).dropna().head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NJfc2gvR2cO-"},"source":["The returned DataFrame now has all of the `NaN` rows dropped. Notice the index now does not include 5.\n","\n","Despite being really handy, `where()` isn't actually used that often. Instead, the pandas developers created a shorthand syntax which combines `where()` and `dropna()` at once. And, in typical fashion, they just overloaded the indexing operator to do this!"]},{"cell_type":"code","metadata":{"id":"83z7yZ3VMUJo"},"source":["df[df['chance of admit'] > 0.7].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"--35taBMDHXJ"},"source":["While I personally find this much harder to read, it's much more commonly used and you're more likely to see it in other people's code, so it's important to be able to understand it. \n","\n","You can use the indexing operator on a DataFrame in three ways:\n","\n","1. It can be called with a string parameter to project a single column\n"]},{"cell_type":"code","metadata":{"id":"tqj8JKeSMUJp"},"source":["df[\"gre score\"].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zlp-Hg2EHx-J"},"source":["2. You can send it a list of columns as strings\n"]},{"cell_type":"code","metadata":{"id":"o-xNx9MYMUJp"},"source":["df[[\"gre score\",\"toefl score\"]].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ts7ScRw4H3-d"},"source":["3. You can send it a boolean mask\n"]},{"cell_type":"code","metadata":{"id":"dWwh0WCEMUJp"},"source":["df[df[\"gre score\"]>320].head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDVw--7COD-g"},"source":["Each of these is mimicing functionality from either `.loc()` or `.where().dropna()`."]},{"cell_type":"markdown","metadata":{"id":"edPfH_TQOSg5"},"source":["###Combining Multiple Boolean Masks\n","\n","Let's look at combining multiple boolean masks. What if we want to satisfy multiple criteria in our selection?\n","\n","This is similar to bitmasking in computer science, where in order to satisfy multuple criteria, we use the \"and\" operator to extract the rows where all the conditions are satified. In the case where at least one of numerous conditions needs to be satisfied, then the \"or\" operator is used.\n","\n","The truth table below summarizes how the \"and\" and \"or\" operators operate:\n","\n","| A | B | A AND B | A OR B |\n","|---|---|---------|--------|\n","|False|False|False|False|\n","|False|True|False|True|\n","|True|False|False|True|\n","|True|True|True|True|\n","\n","\n","Unfortunately, it doesn't feel quite as natural in pandas. For instance, if you want to take two boolean series and \"and\" them together..."]},{"cell_type":"code","metadata":{"id":"sajdU_P-MUJq"},"source":["(df['chance of admit'] > 0.7) and (df['chance of admit'] < 0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r766kikZXoUf"},"source":["... we get an error. And despite using pandas for a while, I still find I regularly try and do this. The problem is that you have series objects, and python underneath doesn't know how to compare two series using \"and\" or \"or\". As a solution, the pandas authors have overwritten the ampersand `&` (instead of \"and\") and pipe `|` (instead of \"or\") operators to handle this."]},{"cell_type":"code","metadata":{"id":"zKAJ88O0MUJq"},"source":["(df['chance of admit'] > 0.7) & (df['chance of admit'] < 0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5P6f52dbRzm"},"source":["A common error for new pandas users is to try and do boolean comparisons using the `&` operator but not putting parentheses around the individual terms you are interested in."]},{"cell_type":"code","metadata":{"id":"w11PilKGMUJr"},"source":["df['chance of admit'] > 0.7 & df['chance of admit'] < 0.9"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tM_LQKKEbdUs"},"source":["The problem is that Python is trying to \"bitwise and\" the `0.7` and a pandas dataframe, when you really want to \"bitwise and\" the two broadcasted dataframes together.\n","\n","\n","Another way to do this is to just get rid of the comparison operator completely, and instead use the built in functions which mimic this approach."]},{"cell_type":"code","metadata":{"id":"f5ovAGsxMUJr"},"source":["df['chance of admit'].gt(0.7) & df['chance of admit'].lt(0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ICMv17HGb6bJ"},"source":["These functions are built right into the Series and DataFrame object. That means that you can chain them too, which results in the same answer without the use of the boolean operators. You can decide what looks best for you."]},{"cell_type":"code","metadata":{"id":"9T6oJj2-MUJr"},"source":["df['chance of admit'].gt(0.7).lt(0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gewjlvyAMUJs"},"source":["In this lecture, we have learned to query dataframe using boolean masking, which is extremely important \n","and often used in the world of data science. With boolean masking, we can select data based on the criteria \n","we desire and, frankly, you'll use it everywhere. We've also seen how there are many different ways to query\n","the DataFrame, and the interesting side implications that come up when doing so."]}]}